<html>
<header>
  <meta http-equiv="content-type" charset="UTF-8">



  <!-- TensorFlow.jsの読み込み -->

  <!-- PoseNetモデルの読み込み -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@2.0.0/dist/tf.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/posenet@2.2.2/dist/posenet.min.js"></script>
  <script src="https://code.jquery.com/jquery-3.6.0.min.js"
    integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>


</header>


<style>
  body {
    height: 90%;
    margin: 0%;
    padding: 0;
  }


  video {
    position: absolute;
    place-content: center;
    height: 80vh;

  }

  canvas {
    position: absolute;
    place-content: center;
    z-index: 10;
    height: 80vh;
  }

  table {
    height: 80vh;
  }

  #wrap {
    height: 80vh;
  }

  #p1 {
    /*カメラ*/
    position: relative;
    place-content: center;
    margin: 5%;
    height: 80vh;
    left: 28vw;
    z-index: 20;
  }


  #form {
    justify-content: space-evenly;
  }

  #load_img {
    width: 90vw;
    height: 90vh;

    /* 以下のコードを追加 */
    position: absolute;
    object-fit: cover;
    top: 0;
    left: 0;
    z-index: 50;
  }

  #go_button {
    position: absolute;
    bottom: 10%;
    right: 10%;
    z-index: 100;
  }

  #board {
    position: absolute;
    /*←絶対位置*/
    bottom: 0;
    /*下に固定*/
    background-color: #fff;
    width: 100%;
    height: 20vh;
    text-align: center;
    z-index: 20;
  }
</style>




<body onload="window_loaded();">
  
  <base href="https://nyatsuyai.github.io/PoseNet_Posture-estimation" target="_self">

  <div id="p1">
    <canvas id="canvas"></canvas>
    <video id="webcam" autoplay playsinline></video>
  </div>

  <img id="load_img" src="img/set_position.jpg">

  <input id="go_button" type="image" src="img/text_yattemiyou.png" onclick="jump_site()">

  <div id="p8">
    <b id="board" style="font-size: 10vw;"></b>
  </div>

  <!-- <input id="howto_button" type="image" src="img/question_head_boy.png" onclick="location.href='./start_site.html'"> -->



</body>
<script>

   'use strict';//ストリクトモードの開始

const jump_url = 'https://script.google.com/macros/s/AKfycbzD7USmdDHqso06unuK-cfholGuBPI4bg-cnIyLB_3NoT_Nu8JBYn3d2Jn3hcOF6snt/exec';
  const kpscore = 0.5//ポイントの判定の厳しさ
  const check_shisei_par = 0.7;//姿勢が悪いと判定する長さ
  const textnumber = 0.85;//スクワット判定する腰膝間の長さ

  const nose = 0;
  const leftEye = 1;
  const rightEye = 2;
  const leftEar = 3;
  const rightEar = 4;
  const leftShoulder = 5;
  const rightShoulder = 6;
  const leftElbow = 7;
  const rightElbo = 8;
  const leftWrist = 9;
  const rightWrist = 10;
  const leftHip = 11;
  const rightHip = 12;
  const leftKnee = 13;
  const rightKnee = 14;
  const leftAnkle = 15;
  const rightAnkle = 16;

  const position_to_index = {
    leftShoulder: 0,
    rightShoulder: 1,
    leftHip: 2,
    rightHip: 3,
    leftKnee: 4,
    rightKnee: 5,
    leftAnkle: 6,
    rightAnkle: 7,
  };


  let pointData = [];
  let flg_Knee = false;
  let shift=0;
  



  const renderToCanvas = async (ctx, a) => {
    const [height, width] = a.shape;
    const imageData = new ImageData(width, height);
    const data = await a.data();
    for (let i = 0; i < height * width; ++i) {
      const j = i * 4;
      const k = i * 3;
      imageData.data[j + 0] = data[k + 0];
      imageData.data[j + 1] = data[k + 1];
      imageData.data[j + 2] = data[k + 2];
      imageData.data[j + 3] = 255;
    }
    //ctx.putImageData(imageData, 0, 0)
  }

  const drawPoint = (ctx, kp) => {
    if (kp.score < kpscore) return
    ctx.fillStyle = 'yellow'
    ctx.beginPath();
    ctx.arc(kp.position.x, kp.position.y, 3, 0, 2 * Math.PI);
    ctx.fill();
  }


  const set_pointData = () => {
    var Dpoont = 0;
    var i;
    for (i = 0; i < 8; i++) {
      Dpoont += pointData[i];
    }
    Dpoont = (Dpoont / 8) * 100;
    // document.getElementById('reliance').innerHTML = Dpoont + "%";
  }

  const keyposition = (kp) => {
    //存在する場合の処理
    if (kp.score < kpscore) pointData[position_to_index[kp.part]] = 0;
    else pointData[position_to_index[kp.part]] = 1;

  }

  const startEstimateSinglePose = () => {
    posenet.load()
      .then(model => {
        const webcamElement = document.getElementById('webcam')
        window.requestAnimationFrame(onFrame.bind(null, model, webcamElement))
      })
  }

  const hantei = (ctx, kp0, kp1) => {
    if (pointData.indexOf(1, 6) === -1) {
      board.style.color = "#ff0000";
      document.getElementById("board").innerHTML = "後ろに下がる";
    } else {
      flg_Knee = true;//膝が見えていることを認証
      board.style.color = "#000000";
      document.getElementById("board").innerHTML = "OK";
    }
  }

  const window_loaded = () => {
    setTimeout(loaded_img(), 5000);
    console.log("a");
  }

  const loaded_img = () => {
    console.log("a");
    setTimeout(function () {
      $("#load_img").animate({
        'opacity': '0'
      }, 1000);
      setTimeout(function () {
        document.getElementById("load_img").style.display = "none";
      }, 1100);
    }, 7000)

  }

  const jump_site=()=>{
location.href = "https://nyatsuyai.github.io/PoseNet_Posture-estimation/posenet_85.html"
  }


  //////////////////////////////////////////////////////////////// フレーム毎に呼ばれる/////////////////////////////////////////////////////////////////////////

  const onFrame = async (model, webcamElement) => {

    //カウント
    //CountTime += 1

    // 姿勢推論
    let tensor = tf.browser.fromPixels(webcamElement)
    // tensor = tensor.resizeBilinear(28,28)
    let ratio = 0.66;
    var vwidth = webcamElement.videoWidth;
    let new_width = Math.min(Math.round(webcamElement.videoHeight * ratio), webcamElement.videoWidth);
    shift = Math.round((vwidth - new_width) / 2);
    ratio = new_width / vwidth;
    tensor = tf.image.cropAndResize(tensor.expandDims(), [[0, shift / vwidth, 1, shift / vwidth + ratio]], [0], [webcamElement.videoHeight, new_width]).squeeze();

    const predictions = await model.estimateSinglePose(tensor, {
      flipHorizontal: false
    })



    // キャンバスの準備
    const canvas = document.getElementById('canvas')
    const [height, width] = tensor.shape

    //キャンバスのサイズ変更
    canvas.width = width
    canvas.height = height

    var video_shift = shift / webcamElement.videoWidth * webcamElement.clientWidth;
    var clip = "rect(0px," + (video_shift + (webcamElement.videoWidth - shift * 2) / webcamElement.videoWidth * webcamElement.clientWidth) + "px," + canvas.clientHeight + "px," + video_shift + "px)";
    var trans = "translate(" + (-video_shift) + "px, 0px)"
    $(function () {
      $("#webcam").css({
        "clip": clip,
        "transform": trans
      });
    });


    // キャンバスの描画
    const ctx = canvas.getContext('2d')
    // await renderToCanvas(ctx, tensor)
    const kp = predictions.keypoints

    //位置表示からの計算準備


    // ポイントの描画
    drawPoint(ctx, kp[nose])
    //drawPoint(ctx, kp[leftEye])
    //drawPoint(ctx, kp[rightEye])
    //drawPoint(ctx, kp[leftEar])
    //drawPoint(ctx, kp[rightEar])
    drawPoint(ctx, kp[leftShoulder])
    keyposition(kp[leftShoulder])
    //drawPoint(ctx, kp[leftElbow])
    //drawPoint(ctx, kp[leftWrist])
    drawPoint(ctx, kp[rightShoulder])
    keyposition(kp[rightShoulder])
    //drawPoint(ctx, kp[rightElbo])
    //drawPoint(ctx, kp[rightWrist])
    drawPoint(ctx, kp[leftHip])
    keyposition(kp[leftHip])
    drawPoint(ctx, kp[rightHip])
    keyposition(kp[rightHip])
    drawPoint(ctx, kp[leftKnee])
    keyposition(kp[leftKnee])
    drawPoint(ctx, kp[leftAnkle])
    keyposition(kp[leftAnkle])
    drawPoint(ctx, kp[rightKnee])
    keyposition(kp[rightKnee])
    drawPoint(ctx, kp[rightAnkle])
    keyposition(kp[rightAnkle])
    set_pointData()



    //判定の設定
    hantei(ctx, kp[leftKnee], kp[rightKnee])








    //console.log(leftShouldery)
    // 次フレーム
    setTimeout(() => {
      window.requestAnimationFrame(onFrame.bind(null, model, webcamElement))
    }, 100)//フレーム数低いと画像数が増える
  }

  // Webカメラの開始
  const constraints = {
    audio: false,
    video: true
  }
  navigator.mediaDevices.getUserMedia(constraints)
    // 成功時に呼ばれる
    .then((stream) => {
      const video = document.querySelector('video')
      video.srcObject = stream

      // 姿勢推定出の開始
      //net
      //startEstimateSinglePose()
    })
    // エラー時に呼ばれる
    .catch((error) => {
      //const errorMsg = document.querySelector('#errorMsg')
      //errorMsg.innerHTML += `<p>${error.name}</p>`
    })

  document.getElementById("webcam").addEventListener('loadeddata', (event) => {
    startEstimateSinglePose()
  });


</script>






</html>

